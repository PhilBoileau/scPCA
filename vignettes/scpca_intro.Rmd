---
title: "Sparse contrastive principal components analysis"
author: "Philippe Boileau"
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document
bibliography: vignette-refs.bib
vignette: >
  %\VignetteIndexEntry{Sparse contrastive principal components analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

```{r setup, echo=FALSE}
library(here)
library(dplyr)
library(ggplot2)
library(ggpubr)
#library(scPCA)
devtools::load_all()
library(elasticnet)
knitr::opts_chunk$set(echo = FALSE)
```

Data pre-processing and exploratory data analysis and are two important steps in
the data science life-cycle. As datasets become larger and the signal weaker,
their importance increases. Thus, methods that are capable of extracting the
signal from such datasets is badly needed. Often, these steps rely on
dimensionality reduction techniques to isolate pertinent information in data.
However, many of the most commonly-used methods fail to reduce the dimensions of
these large and noisy datasets successfully.

Principal component analysis (PCA) is one such method. Although popular for its
interpretable results and ease of implementation, PCA's performance on
high-dimensional often leaves much to be desired. Its results on these large
datasets have been found to be unstable, and it is often unable to identify
variation that is contextually meaningfull.

Fortunately, modifications of PCA have been developed to remedy these issues.
Namely, sparse PCA (sPCA) was created to increase the stability of the principal
component loadings and variable scores in high dimensions, and constrastive PCA
(cPCA) was proposed as a method for capturing relevant information in the
high-dimensional data.

Although sPCA and cPCA have proven useful in resolving individual shortcomings
of PCA, neither is capable of tackling the issues of stability and relavance
simultaneously. The goal of this research project is to determine whether a
combination of these methods, dubbed sparse constrastive PCA (scPCA), can
accomplish this task.

---

# Comparing PCA, SPCA, cPCA and scPCA

A brief comparion of PCA, SPCA, cPCA and scPCA is provided below. All four
methods are applied to a simulated target dataset consisting of `r nrow(toy_df)`
observations and `r ncol(toy_df) - 1` continuous variables. Additionally, each 
observation is classified as belonging to one of four classes. This label is
known is known a priori. A background data set comprised of the same number of
variables as the target data set.

The target data was simulated as follows:

The background data was simulated as follows:

A similar simulation scheme is provided in **Abid et al. (2018)**.

## PCA

First, PCA is applied to the target data. As we can see from the figure, PCA is
incapable of creating a lower dimensional representation of the target data
that captures the variation of interest (i.e. the four groups). In fact, no pair
of principal components among the first twelve were able to.

```{r PCA_sim}
# set seed for reproducibility
set.seed(1742)

# perform PCA
pca_sim <- prcomp(toy_df[, 1:30])

# plot the 2D rep using first 2 components
df <- data.frame("PC1" = pca_sim$x[, 1],
                 "PC2" = pca_sim$x[, 2],
                 "label" = as.character(toy_df[, 31]))
p_pca <- ggplot(df, aes(x = PC1, y = PC2, colour = label)) +
  ggtitle("PCA on Simulated Data") +
  geom_point(alpha = 0.5) +
  theme_minimal()
p_pca
```

## Sparse PCA

Much like PCA, the leading components of SPCA - for varying amounts of
sparsity - are incapable of splitting the observations into four distinct
groups.

```{r sPCA_sim}
# perform sPCA on toy_df for a range of L1 penalty terms
penalties <- exp(seq(log(10), log(1000), length.out = 6))
df_ls <- lapply(penalties, function(penalty) {
  spca_sim_p <- spca(toy_df[, 1:30], K = 2, para = rep(penalty, 2),
                     type = "predictor", sparse = "penalty")$loadings
  spca_sim_p <- as.matrix(toy_df[, 1:30]) %*% spca_sim_p
  spca_out <- list("SPC1" = spca_sim_p[, 1],
                   "SPC2" = spca_sim_p[, 2],
                   "penalty" = round(rep(penalty, nrow(toy_df))),
                   "label"  = as.character(toy_df[, 31])) %>%
    as_tibble()
  return(spca_out)
})
df <- bind_rows(df_ls)

# plot the results of sPCA
p_spca <- ggplot(df, aes(x = SPC1, y = SPC2, colour = label)) +
  geom_point(alpha = 0.5) +
  ggtitle("SPCA on Simulated Data for Varying L1 Penalty Terms") +
  facet_wrap(~ penalty, nrow = 2) +
  theme_minimal()
p_spca
```

## Contrastive PCA (cPCA)

The first two contrastive principal components of cPCA successfully captured
the variation of interest in the data with the help of the background data set.

```{r cPCA_sim}
# run cPCA for using 40 logarithmically seperated contrastive param values
cpca_sim <- scPCA(target = toy_df[, 1:30],
                  background = background_df,
                  penalties = 0,
                  n_centers = 4)

# create a dataframe to be plotted
cpca_df <- cpca_sim$x %>%
             as.data.frame() %>%
             mutate(label = toy_df[, 31] %>% as.character)

colnames(cpca_df) <- c("cPC1", "cPC2", "label")

# plot the results
p_cpca <- ggplot(cpca_df, aes(x = cPC1, y = cPC2, colour = label)) +
  geom_point(alpha = 0.5) +
  ggtitle("cPCA of Simulated Data") +
  theme_minimal()
p_cpca
```

## Sparse and Contrastive PCA (scPCA)

The leading sparse contrastive components were also able to capture the
variation of interest, though the clusters corresponding to the class labels are
more loose than those of cPCA. However, the first and second scPC contain only
two and three non-zero loadings, respectively. This is a significant improvement
over cPCA, whose first and second cPCs each possess 30 non-zero loadings.

```{r scPCA_sim}
# run scPCA for using 40 logarithmically seperated contrastive parameter values
# and possible 20 L1 penalty terms
scpca_sim <- scPCA(target = toy_df[, 1:30],
                  background = background_df,
                  n_centers = 4)

# create a dataframe to be plotted
scpca_df <- scpca_sim$x %>%
             as.data.frame() %>%
             mutate(label = toy_df[, 31] %>% as.character)

colnames(scpca_df) <- c("scPC1", "scPC2", "label")

# plot the results
p_scpca <- ggplot(scpca_df, aes(x = scPC1, y = scPC2, colour = label)) +
  geom_point(alpha = 0.5) +
  theme_minimal()
p_scpca


# create the loadings comparison plot
load_diff_df <- bind_rows(
  cpca_sim$rotation %>% as.data.frame,
  scpca_sim$rotation %>% as.data.frame) %>%
  mutate(
    sparse = c(rep("0", 30), rep("1", 30)),
    coef = rep(1:30, 2)
  )

colnames(load_diff_df) <- c("comp1", "comp2", "sparse", "coef")

p1 <- load_diff_df %>% ggplot(aes(y = comp1, x = coef, fill = sparse)) +
  geom_bar(stat = "identity") +
  xlab("") +
  ylab("") +
  ylim(-1, 1) +
  ggtitle("First Component") +
  scale_fill_discrete(name = "Method",
                      labels = c("cPCA", "scPCA"))+
  theme_minimal()

p2 <- load_diff_df %>% ggplot(aes(y = comp2, x = coef, fill = sparse)) +
  geom_bar(stat = "identity") +
  xlab("") +
  ylab("") +
  ylim(-1, 1) +
  ggtitle("Second Component") +
  scale_fill_discrete(name = "Method",
                      labels = c("cPCA", "scPCA"))+
  theme_minimal()


annotate_figure(
  ggarrange(p1, p2, nrow = 1, ncol = 2,
            common.legend = TRUE, legend = "right"),
  top = "Non-Zero Loadings Comparison",
  left = "Loading Values",
  bottom = "Variable Index"
)
```



---

## Bioconductor Integration with `SingleCellExperiment`

We now turn to discussing how the tools in the `scPCA` package can be used more
readily with data structures common in computational biology by examining an
integration with the `SingleCellExperiment` container class. For our example, we
will use the `allen` data from the `scRNAseq` package, just as was done to
demonstrate use of the `SingleCellExperiment` container in the corresponding
vignette.

To start, let's load the required packages and create a simple
`SingleCellExperiment` instance, including information about spike-ins:

```{r sce_setup, message=FALSE}
library(scRNAseq)
library(SingleCellExperiment)

# load data and set to SCE
data(allen)
sce <- as(allen, "SingleCellExperiment")
```

While interesting, the `allen` data set is rather large. To proceed, we use only
a small subset of the data, limited to the 1000 genes with the highest variance,
after log-transforming the raw counts:

```{r make_sce_sub, message=FALSE}
# rank genes by variance
n_genes <- 1000
vars <- assay(sce) %>%
  log1p %>%
  rowVars
names(vars) <- rownames(sce)
vars <- sort(vars, decreasing = TRUE)

# subset SCE to n_genes with highest variance
sce_sub <- sce[names(vars[seq_len(n_genes)]),]
sce_sub
```

In order to use either contrastive PCA or its sparse variant, it is necessary to
have a background data set. Absent such a version of the `allen` data, we will
use the `r n_genes` genes with the lowest variance as the background:

```{r make_sce_background, message=FALSE}
# background SCE based on low variance genes (after removing zero counts)
vars_nz <- vars[vars > 0]
vars_low <- vars[(length(vars_nz) - n_genes + 1):length(vars_nz)]
sce_background <- sce[names(vars_low), ]
```

Due to the flexibility of the `SingleCellExperiment` class, we can obtain PCA,
cPCA, and scPCA representations of the reduced `allen` data, storing these in
 `SingleCellExperiment` object using the `reducedDims` method.

```{r perform_dimred, message=FALSE}
# first, PCA
pca_data <- prcomp(t(log1p(assay(sce_sub))))

# next, cPCA
cpca_data <- cPCA(t(log1p(assay(sce_sub))),
                  t(log1p(assay(sce_background))),
                  center = TRUE, scale = FALSE)

# finally, scPCA
scpca_data <- scPCA(t(log1p(assay(sce_sub))),
                    t(log1p(assay(sce_background))),
                    num_medoids = 20,
                    scale = FALSE)

# store new representations in the SingleCellExperiment object
reducedDims(sce_sub) <- SimpleList(PCA = pca_data$x, cPCA = cpca_data$Y,
                                   scPCA = ...)
sce_sub
```

---

## Session Information

```{r session_info, echo=FALSE}
sessionInfo()
```

---

## References

